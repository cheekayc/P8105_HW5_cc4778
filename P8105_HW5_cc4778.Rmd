---
title: "Homework 5"
author: "Chee Kay Cheong"
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  fig.width = 12,
  fig.height = 8,
  out.width = "90%")

library(tidyverse)
library(fs) # Load this to use "dir_ls" and map function after that.
library(purrr)
```

# Problem 1

Create a list to read and store all the csv files located in the *Data* folder.
```{r map funstion to read multiple csv files}
# I first create a directory that contains the path to all the csv files stored in the Data folder.
Data = "C:/Users/Chu Chu/Desktop/R/Homework/HW5/Problem 1 Data/"

# Then I create a list that will contain all the csv file from the Data folder.
study_list = 
  Data %>% 
  # The "dir_ls" function returns filenames as a named fs_path character vector. 
  # The names are equivalent to the values, which is useful for passing onto functions like purrr::map_df().
  dir_ls() %>% 
  map(
    # Then, map a function that will read csv files.
    .f = function(path) {
      read_csv(path)
    }
  )
```

Create a single dataframe that combines all data in each csv files.
```{r bind rows to make one tidy dataframe}
# I create a dataframe that read the list of csv files and bind all the rows into one dataframe.
long_study = 
  study_list %>% 
  # "set_names" allows R to show all the columns and values from the list of csv files.
  set_names(dir_ls(Data)) %>% 
  # Then, bind all of them together to make one tidy dataframe.
  bind_rows(.id = "file_path") %>% 
  # Create two variables "arm" & "subject_ID" for each observation.
  mutate(
    arm = str_replace(file_path, "^C:/Users/Chu Chu/Desktop/R/Homework/HW5/Problem 1 Data/", ""),
    arm = str_replace(arm, ".csv$", "")) %>% 
    separate(arm, c('arm', 'subject_ID')) %>% 
  select(arm, subject_ID, week_1:week_8, -file_path) %>% 
  # pivot_longer so we can do some analysis later
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "observations",
    names_prefix = "week_")

long_study
```

```{r}
long_study %>% 
  group_by(arm, subject_ID) %>% 
  summarize(
    n_obs = n())
```


The `long_study` dataset contains 2 arms (experimental & control). Each arm contains 10 subjects, and each subject contributes 8 observations 
over the period of 8 weeks (1 observation per week for each subject).

Everything looks fine and data are cleaned, but I don't understand what the values under `observations` are representing.

```{r spaghetti plot}
Arms_name = c(
  `con` = "Control",
  `exp` = "Experimental")

long_study %>% 
  ggplot(aes(x = week, y = observations, group = subject_ID, color = subject_ID)) +
  geom_line() + 
  stat_summary(aes(group = 1), geom = "point", fun.y = mean, shape = 17, size = 3) +
  facet_grid( . ~ arm, labeller = as_labeller(Arms_name)) +
  labs(
    x = "Week",
    y = "Observations",
    title = "Observations over time graph") +
  theme_minimal() + theme(legend.position = "bottom")
```

Based on the spaghetti plot, the overall observations over 8 weeks of time are higher in the experimental group when comparing to the control group.

In the experimental group, the mean observations are increasing with time, whereas the mean observations in the control group do not have much changes with time. 


# Problem 2

##### Raw data

```{r read raw dataset}
homicide_raw = read_csv("./Data/homicide_data.csv") 
```

The raw dataset `homicide_raw` consists of `r ncol(homicide_raw)` variables (*`r names(homicide_raw)`*) and `r nrow(homicide_raw)` observations.                                                            
Missing observations in the latitude `lat` and longitude `lon` columns are identified as `NA`, while all other columns have some `Unknown` observations.                                                    
Most variables are character vectors, except for `reported_date`, `lat`, and `lon`, which are numeric vector.                                                                                                
I noticed that the dates are recorded in *yyyymmdd* format, which are not in typical date format and are not arranged in sequence. 
Besides, `victim_age` has been identified as character vector due to several `Unknown` under the column. 

Therefore, I would convert the `reported_date` to a nice date format and convert the `victim_age` variable into a numeric vector.

##### Cleaned data

```{r clean dataset}
homicides =
  homicide_raw%>% 
  janitor::clean_names() %>% 
  mutate(
    victim_age = str_replace(victim_age, "Unknown", "NA"),
    victim_age = as.numeric(victim_age),
    reported_date = as.character(reported_date),
    reported_date = as.Date(reported_date, format = "%Y%m%d"),
    state = ifelse(city == "Tulsa", "OK", state),
    state = str_replace(state, "w", "W")) %>%
  unite(col = 'city_state', c('city', 'state'), sep = ', ')
```

##### Number of homicides & number of unsolved homicides in 50 cities

The table below shows the number of homicides and the number of unsolved homicides in each city.
```{r}
homicides %>% 
  group_by(city_state) %>% 
  summarize(
    n_case = n(),
    n_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))) %>% 
  knitr::kable()
```

From the output, I noticed that there was an error in the `homicides` dataset. There is a `city_state` named *Tulsa, Alabama* 
that has only 1 case of homicide but has 0 case of unsolved homicide. I googled and checked if there is a city named *Tulsa* 
in *Alabama*. I found that there is only one place named *Tulsa* in America, and that is in *Oklahoma*. Hence, I am pretty 
sure that there is an error in the data and decided to fix that error by changing `Tulsa, AL` to `Tulsa, OK`.

I also noticed another minor issue with the dataset from the table above - `Milwaukee, wI`.
Although it probably is not a big issue, but to keep my dataset nice and tidy, I decided to make the lowercase "w" to uppercase "W".

I will make all the changes in the code chunk named `clean dataset` above, so I am not repeating the codes and starting another 
code chunk. 


##### Baltimore

```{r Baltimore}
Baltimore = 
  homicides %>% 
  filter(city_state == "Baltimore, MD") %>% 
  group_by(city_state) %>% 
  summarize(
    n_case = n(),
    n_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))

x_balti = Baltimore %>% pull(n_unsolved)

n_balti = Baltimore %>% pull(n_case)

list_baltimore = 
  prop.test(x_balti, n_balti) %>% 
  broom::tidy()

Baltimore_stat = 
  bind_cols(Baltimore, list_baltimore)

Baltimore_stat %>% 
  select(city_state, n_case, n_unsolved, estimate, conf.low, conf.high) %>% 
  knitr::kable()
```

In Baltimore, MD, it is estimated that **64.56%** of homicides remained unsolved. We are 95% confident that the proportion of 
unsolved homicides in Baltimore, MD lies between *0.63* and *0.66*.


##### All cities

I will be creating a neat dataframe that shows the number of homicide cases, the number of unsolved homicide cases, the proportion
of unsolved homicides and their corresponding 95% confidence interval in each of the 50 major cities.

```{r a neat dataframe to be used}
all_cities = 
  homicides %>% 
  group_by(city_state) %>% 
  summarize(
    n_case = n(),
    n_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))) %>% 
  nest(data = n_case:n_unsolved)
```

```{r write a function}
prop_unsolved = function(df) {

  x = df %>% pull(n_unsolved)
  
  n = df %>% pull(n_case)
  
  results = 
    prop.test(x, n) %>% 
    broom::tidy()

  result_stat = 
    bind_cols(df, results)

  result_stat %>% 
    select(estimate, conf.low, conf.high)
  
}
```

```{r apply function}
all_cities_stat = 
  all_cities %>% 
  mutate(
    map_df(all_cities[["data"]], prop_unsolved)) %>% 
  unnest(data)

all_cities_stat
```

##### Create a plot

Create a plot that shows the estimated proportion of unsolved homicides and their corresponding 95% confidence intervals for each city.

```{r bar plot with error bar}
all_cities_stat %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = estimate, y = city_state, fill = city_state)) +
  geom_bar(stat="identity") +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high), width = 0.5) +
  labs(
    x = "Proportion of unsolved homicide",
    y = "City, State",
    title = "Proportion of unsolved homicide with 95% CI in 50 major cities in the US") +
  theme_minimal() +
  theme(legend.position = "none")
```


# Problem 3

Create a list of 5000 dataframes, which each of them is comprised of 30 random numbers that follow a normal distribution of 
mean = 0 and standard deviation = 5.

```{r create a list of dataframes}
first_set = vector(mode = 'list', length = 5000)
  n = 5000
  for (i in 1:n) {
    first_set[[i]] = rnorm(30, 0, 5)
  }
```

Create a function to obtain the estimated mean of each sample and their corresponding p-value.
```{r create a function for list}
samp_mean_p = function(x) {
  
  samp_results = t.test(x, mu = 0) %>% 
    broom::tidy() %>% 
    select(estimate, p.value) %>% 
    rename(
      mean_hat = estimate,
      p_value = p.value)
}
```

Apply function to the list and store the output in a nice dataframe.
```{r apply function to first list}
first_list_df = 
  tibble(
  sample = first_set)

test_one = map_df(first_set, samp_mean_p)

First_set_tidy = bind_cols(first_list_df, test_one)

First_set_tidy
```

#### Different values of true mean

Repeat the above for μ = {1,2,3,4,5,6}. Just in case we need to include that one dataframe we created above (true mean = 0). 
I am going to include true mean = 0 as well in the iteration below.

Write a function:
```{r create function for mean change}
change_mean = function(mu) {
  
  x = rnorm(30, mu, 5)
  
  samp_results = t.test(x, mu = 0) %>% 
    broom::tidy() %>% 
    select(estimate, p.value) %>% 
    rename(
      mean_hat = estimate,
      p_value = p.value) %>% 
    mutate(
      random_x = list(x),
      sample_size = 30,
      true_sd = 5)
}
```

Create a nice and tidy dataframe:
```{r create dataframe for mean change}
multiple_mean = 
  expand_grid(   
    true_mean = c(0, 1, 2, 3, 4, 5, 6), 
    iteration = 1:5000) %>%   
  mutate(
    estimate_measures = map(true_mean, change_mean)) %>% 
  unnest(estimate_measures)

multiple_mean =
  multiple_mean %>% 
  select(random_x, sample_size, true_sd, true_mean, mean_hat, p_value)

multiple_mean
```

##### Plot 1: Power vs. Effect size

```{r plot1}
multiple_mean %>% 
  group_by(true_mean) %>% 
  summarize(
    prop_reject = sum(p_value < 0.05) / 5000) %>% 
  ggplot(aes(x = true_mean, y = prop_reject)) +
  geom_point() +
  geom_line() + 
  labs(
    x = "True mean",
    y = "Proportion",
    title = "Proportion of rejecting H0 for each sample with different true mean") +
  theme_minimal()
```

As we can see from the plot above, the proportion of times the null was rejected increases as the value of true mean increases.
In other words, when the value of true mean is further away from the null (μ = 0), meaning the effect size increases, the power also increases.


##### Plot 2: 

```{r plot2, fig.height = 10, fig.width = 11}
ave_all =
  multiple_mean %>% 
  group_by(true_mean) %>% 
  summarize(
    average_mean_hat = mean(mean_hat))

ave_rej = 
  multiple_mean %>% 
  filter(p_value < 0.05) %>% 
  group_by(true_mean) %>% 
  summarize(
    average_mean_hat = mean(mean_hat))

ggplot(ave_all, aes(x = true_mean, y = average_mean_hat, color = "All samples")) +
  geom_point() +
  geom_line() + 
  geom_line(data = ave_rej, aes(x = true_mean, y = average_mean_hat, color = "Reject H0 samples"), alpha = 0.4) +
  geom_point(data = ave_rej, aes(x = true_mean, y = average_mean_hat, color = "Reject H0 samples"), alpha = 0.4) +
  labs(
    x = "True mean",
    y = "Average sample mean",
    title = "Average sample means vs. True means") +
  theme_minimal()
```

Based on the graph above, the sample average means when taking all samples (*the red line*) are approximately equal to the true value of mean (μ).

Looking at the *blue line* in which the sample average means are computed based **only** on the samples that rejected the null hypothesis (H0: μ = 0),
we can see that average sample mean only closely approximate the true mean when the value of sample average mean increases, specifically after the mean 
value of 3. It kind of make sense because when our true mean values are small (in this case < 4), the sample means must be large enough (greater than 0) 
to reject the null hypothesis. 

Take the mean value of 1 as an example, because the effect size is so small, when we filtered out the samples that failed
to reject the null hypothesis, the remaining mean values must be large enough. That is why we see the sample average mean is somewhere around 2.25 when
the true mean is 1.

In contrast, when the effect size is large enough (eg. mean = 4), almost all of samples rejected the null hypothesis. This claim can be supported by plot 1
where the proportion of rejecting H0 when the true mean value equals to 4 is approximately 1. In this case, not many samples will be filtered out, and the 
average sample mean will closely approximate the true mean.
